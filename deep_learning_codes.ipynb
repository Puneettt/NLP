{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d446e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "#from textblob import Word\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "#import wordcloud\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130194ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "209dd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('amazon_alexa.tsv', sep = '\\t')\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "da2be4f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a one of the best apps acording to a b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a pretty good version of the game for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a really cool game. there are a bunch ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a silly game and can be frustrating, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a terrific game on any pad. Hrs of fun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  Positive\n",
       "0  This is a one of the best apps acording to a b...         1\n",
       "1  This is a pretty good version of the game for ...         1\n",
       "2  this is a really cool game. there are a bunch ...         1\n",
       "3  This is a silly game and can be frustrating, b...         1\n",
       "4  This is a terrific game on any pad. Hrs of fun...         1"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d1322ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewText    0\n",
       "Positive      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if there are any null values\n",
    "data = data[data['reviewText'].notnull()]\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "44ee934e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 4)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "771719b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive\n",
       "1    0.76165\n",
       "0    0.23835\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Positive.value_counts()/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3271d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleaning(df, stop_words):\n",
    "#     from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "#     #tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#     df['verified_reviews'] = df['verified_reviews'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
    "#     #df['verified_reviews'] = df['verified_reviews'].apply(lambda x: ' '.join(tokenizer.tokenize(x.lower())))\n",
    "#     # Replacing the special characters\n",
    "\n",
    "#     df['verified_reviews'] = df['verified_reviews'].str.replace('[^ws]', '')\n",
    "\n",
    "#     # Replacing the digits/numbers\n",
    "\n",
    "#     df['verified_reviews'] = df['verified_reviews'].str.replace('d', '')\n",
    "\n",
    "#     # Removing stop words\n",
    "\n",
    "#     df['verified_reviews'] = df['verified_reviews'].apply(lambda x:' '.join(x for x in x.split() if x not in stop_words))\n",
    "\n",
    "#     # Lemmatization\n",
    "\n",
    "#     df['verified_reviews'] = df['verified_reviews'].apply(lambda x:' '.join([Word(x).lemmatize() for x in x.split()]))\n",
    "#     return df\n",
    "\n",
    "# stop_words = stopwords.words('english')\n",
    "# data_v1 = cleaning(data_v1, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "83330e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df = data.copy()\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: [i for i in x if i not in stopwords.words('english')])\n",
    "df['reviewText'] = df['reviewText'].apply(lambda x: [WordNetLemmatizer().lemmatize(i) for i in x])\n",
    "df['sentiment'] = df['reviewText'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "be3aface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Positive</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, best, apps, acording, bunch, people, agr...</td>\n",
       "      <td>1</td>\n",
       "      <td>one best apps acording bunch people agree bomb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[pretty, good, version, game, free, lot, diffe...</td>\n",
       "      <td>1</td>\n",
       "      <td>pretty good version game free lot different le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[really, cool, game, bunch, level, find, golde...</td>\n",
       "      <td>1</td>\n",
       "      <td>really cool game bunch level find golden egg s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[silly, game, frustrating, lot, fun, definitel...</td>\n",
       "      <td>1</td>\n",
       "      <td>silly game frustrating lot fun definitely reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[terrific, game, pad, hr, fun, grandkids, love...</td>\n",
       "      <td>1</td>\n",
       "      <td>terrific game pad hr fun grandkids love great ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  Positive  \\\n",
       "0  [one, best, apps, acording, bunch, people, agr...         1   \n",
       "1  [pretty, good, version, game, free, lot, diffe...         1   \n",
       "2  [really, cool, game, bunch, level, find, golde...         1   \n",
       "3  [silly, game, frustrating, lot, fun, definitel...         1   \n",
       "4  [terrific, game, pad, hr, fun, grandkids, love...         1   \n",
       "\n",
       "                                           sentiment  \n",
       "0  one best apps acording bunch people agree bomb...  \n",
       "1  pretty good version game free lot different le...  \n",
       "2  really cool game bunch level find golden egg s...  \n",
       "3  silly game frustrating lot fun definitely reco...  \n",
       "4  terrific game pad hr fun grandkids love great ...  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5aefa0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          reviewText  Positive  \\\n",
      "0  [one, best, apps, acording, bunch, people, agr...         1   \n",
      "1  [pretty, good, version, game, free, lot, diffe...         1   \n",
      "2  [really, cool, game, bunch, level, find, golde...         1   \n",
      "3  [silly, game, frustrating, lot, fun, definitel...         1   \n",
      "4  [terrific, game, pad, hr, fun, grandkids, love...         1   \n",
      "\n",
      "                                    verified_reviews  \\\n",
      "0  This is a one of the best apps acording to a b...   \n",
      "1  This is a pretty good version of the game for ...   \n",
      "2  this is a really cool game. there are a bunch ...   \n",
      "3  This is a silly game and can be frustrating, b...   \n",
      "4  This is a terrific game on any pad. Hrs of fun...   \n",
      "\n",
      "                                           sentiment  scores  \n",
      "0  one best apps acording bunch people agree bomb...       1  \n",
      "1  pretty good version game free lot different le...       1  \n",
      "2  really cool game bunch level find golden egg s...       1  \n",
      "3  silly game frustrating lot fun definitely reco...       1  \n",
      "4  terrific game pad hr fun grandkids love great ...       1  \n",
      "[[ 1063  3704]\n",
      " [  536 14697]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.22      0.33      4767\n",
      "           1       0.80      0.96      0.87     15233\n",
      "\n",
      "    accuracy                           0.79     20000\n",
      "   macro avg       0.73      0.59      0.60     20000\n",
      "weighted avg       0.77      0.79      0.75     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Sentiment Analyzer\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "'''\n",
    "Sentiment analysis is a technique used to determine the emotional tone or sentiment expressed in a text. It involves analyzing \n",
    "the words and phrases used in the text to identify the underlying sentiment, whether it is positive, negative, or neutral.\n",
    "\n",
    "Good for short text. \n",
    "\n",
    "'''\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyser(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    sentiment = 1 if scores['pos'] > 0 else 0\n",
    "    return sentiment\n",
    "\n",
    "# Apply this to our dataframe\n",
    "\n",
    "df['scores'] = df['sentiment'].apply(analyser)\n",
    "print(df.head())\n",
    "\n",
    "## Classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(confusion_matrix(df['Positive'], df['scores']))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df['Positive'], df['scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "5571be02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Positive</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, best, apps, acording, bunch, people, agr...</td>\n",
       "      <td>1</td>\n",
       "      <td>one best apps acording bunch people agree bomb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[pretty, good, version, game, free, lot, diffe...</td>\n",
       "      <td>1</td>\n",
       "      <td>pretty good version game free lot different le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[really, cool, game, bunch, level, find, golde...</td>\n",
       "      <td>1</td>\n",
       "      <td>really cool game bunch level find golden egg s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[silly, game, frustrating, lot, fun, definitel...</td>\n",
       "      <td>1</td>\n",
       "      <td>silly game frustrating lot fun definitely reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[terrific, game, pad, hr, fun, grandkids, love...</td>\n",
       "      <td>1</td>\n",
       "      <td>terrific game pad hr fun grandkids love great ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  Positive  \\\n",
       "0  [one, best, apps, acording, bunch, people, agr...         1   \n",
       "1  [pretty, good, version, game, free, lot, diffe...         1   \n",
       "2  [really, cool, game, bunch, level, find, golde...         1   \n",
       "3  [silly, game, frustrating, lot, fun, definitel...         1   \n",
       "4  [terrific, game, pad, hr, fun, grandkids, love...         1   \n",
       "\n",
       "                                           sentiment  \n",
       "0  one best apps acording bunch people agree bomb...  \n",
       "1  pretty good version game free lot different le...  \n",
       "2  really cool game bunch level find golden egg s...  \n",
       "3  silly game frustrating lot fun definitely reco...  \n",
       "4  terrific game pad hr fun grandkids love great ...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e48487",
   "metadata": {},
   "source": [
    "### Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "537a56dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>app</td>\n",
       "      <td>10879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6415</th>\n",
       "      <td>game</td>\n",
       "      <td>6899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9070</th>\n",
       "      <td>love</td>\n",
       "      <td>4302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6813</th>\n",
       "      <td>great</td>\n",
       "      <td>4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8850</th>\n",
       "      <td>like</td>\n",
       "      <td>4042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     features  counts\n",
       "1187      app   10879\n",
       "6415     game    6899\n",
       "9070     love    4302\n",
       "6813    great    4092\n",
       "8850     like    4042"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "count_vectoriser = CountVectorizer()                                  ## countvectorizer model object\n",
    "count_vectoriser2 = count_vectoriser.fit_transform(df['sentiment']) ## create sparse matrix of documents and vector counts\n",
    "\n",
    "features = count_vectoriser.get_feature_names_out() #features\n",
    "\n",
    "df_sparse = pd.DataFrame(count_vectoriser2.todense(), columns = features)\n",
    "\n",
    "feature_counts = np.sum(df_sparse.values, axis = 0)\n",
    "\n",
    "feature_counts_df = pd.DataFrame(dict(features = features, counts = feature_counts))\n",
    "feature_counts_df.sort_values(by = 'counts', ascending=False, inplace = True); feature_counts_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000fd69f",
   "metadata": {},
   "source": [
    "### Naive-Bayes Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2e0c1c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>app</td>\n",
       "      <td>10879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>game</td>\n",
       "      <td>6899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  features  counts\n",
       "0      app   10879\n",
       "1     game    6899"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "countvect_tuned = CountVectorizer(stop_words='english', min_df=0.2, max_df=0.5, ngram_range=(1, 3)) \n",
    "#countvect_tuned = CountVectorizer(stop_words='english') \n",
    "\n",
    "#min_df - ignores tokens which are present in less than 20% documents.\n",
    "#max_df - ignores tokens which are present in more than 50% of the document.\n",
    "#max_features - limiting vocabulary size to 50. Selectly mots common 50 tokens.\n",
    "#binary - if set to False, then counts aren't taken only 1 and 0 are imputed.\n",
    "#processor = noise removal function etc.\n",
    "\n",
    "countvect_tuned_vector = countvect_tuned.fit_transform(df['sentiment'])\n",
    "\n",
    "features = countvect_tuned.get_feature_names_out() #features\n",
    "\n",
    "df_sparse = pd.DataFrame(countvect_tuned_vector.todense(), columns = features)\n",
    "\n",
    "feature_counts = np.sum(df_sparse.values, axis = 0)\n",
    "feature_counts_df = pd.DataFrame(dict(features = features, counts = feature_counts))\n",
    "feature_counts_df.sort_values(by = 'counts', ascending=False, inplace = True); feature_counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "972f0ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.76\n",
      "Test accuracy: 0.763\n",
      "[[   3  944]\n",
      " [   4 3049]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.00      0.01       947\n",
      "           1       0.76      1.00      0.87      3053\n",
      "\n",
      "    accuracy                           0.76      4000\n",
      "   macro avg       0.60      0.50      0.44      4000\n",
      "weighted avg       0.68      0.76      0.66      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(countvect_tuned_vector.toarray(), df['Positive'], test_size=0.2, random_state=1)\n",
    "nb_clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print('Train accuracy: {}'.format(accuracy_score(y_train, nb_clf.predict(X_train))))\n",
    "print('Test accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    " \n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169dd8c4",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e616a5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7678125\n",
      "Test accuracy: 0.764\n",
      "[[ 813  134]\n",
      " [ 810 2243]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.86      0.63       947\n",
      "           1       0.94      0.73      0.83      3053\n",
      "\n",
      "    accuracy                           0.76      4000\n",
      "   macro avg       0.72      0.80      0.73      4000\n",
      "weighted avg       0.84      0.76      0.78      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer   \n",
    "\n",
    "idf_vect = TfidfVectorizer(stop_words='english', min_df=0.01, max_df=.7, ngram_range=(1, 3))                            \n",
    "idf_vect_2 = idf_vect.fit_transform(df['sentiment']) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(idf_vect_2.toarray(), df['Positive'], test_size=0.2, random_state=1)\n",
    "nb_clf = GaussianNB().fit(X_train, y_train)\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print('Train accuracy: {}'.format(accuracy_score(y_train, nb_clf.predict(X_train))))\n",
    "print('Test accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    " \n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d9651250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>Positive</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[one, best, apps, acording, bunch, people, agr...</td>\n",
       "      <td>1</td>\n",
       "      <td>one best apps acording bunch people agree bomb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[pretty, good, version, game, free, lot, diffe...</td>\n",
       "      <td>1</td>\n",
       "      <td>pretty good version game free lot different le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[really, cool, game, bunch, level, find, golde...</td>\n",
       "      <td>1</td>\n",
       "      <td>really cool game bunch level find golden egg s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[silly, game, frustrating, lot, fun, definitel...</td>\n",
       "      <td>1</td>\n",
       "      <td>silly game frustrating lot fun definitely reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[terrific, game, pad, hr, fun, grandkids, love...</td>\n",
       "      <td>1</td>\n",
       "      <td>terrific game pad hr fun grandkids love great ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  Positive  \\\n",
       "0  [one, best, apps, acording, bunch, people, agr...         1   \n",
       "1  [pretty, good, version, game, free, lot, diffe...         1   \n",
       "2  [really, cool, game, bunch, level, find, golde...         1   \n",
       "3  [silly, game, frustrating, lot, fun, definitel...         1   \n",
       "4  [terrific, game, pad, hr, fun, grandkids, love...         1   \n",
       "\n",
       "                                           sentiment  \n",
       "0  one best apps acording bunch people agree bomb...  \n",
       "1  pretty good version game free lot different le...  \n",
       "2  really cool game bunch level find golden egg s...  \n",
       "3  silly game frustrating lot fun definitely reco...  \n",
       "4  terrific game pad hr fun grandkids love great ...  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf54b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bce034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac34b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "732d9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_words=''\n",
    "# for i in df.reviewText:\n",
    "#     i = str(i)\n",
    "#     tokens = i.split()\n",
    "#     common_words += \" \".join(tokens)+\" \"\n",
    "# wordcloud = wordcloud.WordCloud().generate(common_words)\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2c44c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoded the target column\n",
    "# lb=LabelEncoder()\n",
    "# df['sentiment'] = lb.fit_transform(df['Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3700eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0c9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a1f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d45c338",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "6cd65469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16833\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None, split=' ')               # It tokenizes by taking all unique values from the training text.\n",
    "tokenizer.fit_on_texts(df['sentiment'].values)    # fit it on the text\n",
    "X = tokenizer.texts_to_sequences(df['sentiment'].values)  # get corresponding sequnce number for the tokens\n",
    "X = pad_sequences(X)                                                  # make the sequence to equal length with zeroes at the starting\n",
    "\n",
    "#from keras.preprocessing.text import text_to_word_sequence - to split on spaces and lowrcase, remove punctuations,\n",
    "#tokens = text_to_word_sequence(text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe58518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa1d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f332c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a26bb978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 42)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1626dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0   63    3 4327\n",
      "  4327]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences([\"physics is nice jnhkjl love echo echo\"]) # .\n",
    "text = pad_sequences(sequences, maxlen=15, padding = 'pre')  # pre means filling zero at first and post means doing it at end.\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a1bac6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2eabccc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   798,  8036,   246],\n",
       "       [    0,     0,     0, ...,    88,    91,    30],\n",
       "       [    0,     0,     0, ...,  1451,   316,    12],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,    25,    12,    12],\n",
       "       [    0,     0,     0, ...,   508,   299,    13],\n",
       "       [    0,     0,     0, ...,   182, 16833,   733]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "8ae35ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,  400,    2,  545,   15,  408,\n",
       "          5,   17,   12,    2, 1659, 2248,    2,  126,  261])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1d21a420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d5ae86a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Splitting the data into training and testing\n",
    "y=pd.get_dummies(df['Positive'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c596d2",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "58d1585b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   30,   36,  246],\n",
       "       [   0,    0,    0, ..., 5338,  105,    5],\n",
       "       [   0,    0,    0, ...,   38,  171,  109],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   43,    9, 5520],\n",
       "       [   0,    0,    0, ...,   19,  859,  711],\n",
       "       [   0,    0,    0, ...,  232, 2156, 2206]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007a0a7",
   "metadata": {},
   "source": [
    "### SimpleRNN - No weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "91ec0483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p192\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_24\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_24\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">184,830</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">612</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_24 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │       \u001b[38;5;34m5,050,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ simple_rnn_4 (\u001b[38;5;33mSimpleRNN\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m)                 │         \u001b[38;5;34m184,830\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m612\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,235,642</span> (19.97 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,235,642\u001b[0m (19.97 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,235,642</span> (19.97 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,235,642\u001b[0m (19.97 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 145ms/step - accuracy: 0.7289 - loss: 0.5848\n",
      "Epoch 2/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 93ms/step - accuracy: 0.8191 - loss: 0.4032\n",
      "Epoch 3/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 92ms/step - accuracy: 0.9126 - loss: 0.2226\n",
      "Epoch 4/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 89ms/step - accuracy: 0.9378 - loss: 0.1630\n",
      "Epoch 5/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 88ms/step - accuracy: 0.9539 - loss: 0.1262\n",
      "Simple RNN LSTM Performance\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8558 - loss: 0.3903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3898458480834961, 0.8544999957084656]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300, input_shape = (X.shape[1],)))\n",
    "\n",
    "#Embedding layer learns weights along it's way.And converts discrete data to real valued densed vector.\n",
    "\n",
    "model.add(SimpleRNN(305, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# predict if 1 or 0. Binary \n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "# compile the model \n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Fitting on train\n",
    "batch_size=32\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 'auto')\n",
    "\n",
    "print('Simple RNN LSTM Performance')\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e38e35",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54f06d",
   "metadata": {},
   "source": [
    "### LSTM - No weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "743d9a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p192\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_25\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_25\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout1d_6                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">739,320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">612</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_25 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │       \u001b[38;5;34m5,050,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout1d_6                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m)                 │         \u001b[38;5;34m739,320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m612\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,790,132</span> (22.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,790,132\u001b[0m (22.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,790,132</span> (22.09 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,790,132\u001b[0m (22.09 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 180ms/step - accuracy: 0.8235 - loss: 0.3997\n",
      "Epoch 2/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 194ms/step - accuracy: 0.9350 - loss: 0.1747\n",
      "Epoch 3/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 190ms/step - accuracy: 0.9575 - loss: 0.1188\n",
      "Epoch 4/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 205ms/step - accuracy: 0.9698 - loss: 0.0813\n",
      "Epoch 5/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 203ms/step - accuracy: 0.9822 - loss: 0.0539\n",
      "LSTM Performance\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 68ms/step - accuracy: 0.8808 - loss: 0.4613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48737993836402893, 0.878000020980835]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300, input_shape = (X.shape[1],)))\n",
    "# takes in input length 500, it can be no longer than the highest tokenized index that is 500 or max unique values. 2d tensor (batch, input_dim)\n",
    "# outputs output of length 120, 3D tensor (batch, input_shape or max_text_length, output_dim). Batch = None\n",
    "\n",
    "# It will output texts with max_text_length size each embedded to 120 dimensions.\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "# inputs 3D tensor, drops 40% input units.\n",
    "\n",
    "model.add(LSTM(305, dropout=0.2, recurrent_dropout=0.2))\n",
    "# 3D tensor now fed to LSTM with 176 units. Returns (Batch, 176 outputs)\n",
    "# 20% Fraction of the units to drop for the linear transformation of the inputs.\n",
    "# 20% Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# predict if 1 or 0. Binary \n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "# compile the model \n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Fitting on train\n",
    "batch_size=32\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 'auto')\n",
    "\n",
    "print('LSTM Performance')\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443857d",
   "metadata": {},
   "source": [
    "### GRU - NO WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6db9f0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p192\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_26\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_26\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout1d_7                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">305</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">555,405</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">612</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_26 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │       \u001b[38;5;34m5,050,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout1d_7                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m305\u001b[0m)                 │         \u001b[38;5;34m555,405\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m612\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,606,217</span> (21.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,606,217\u001b[0m (21.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,606,217</span> (21.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,606,217\u001b[0m (21.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 156ms/step - accuracy: 0.8180 - loss: 0.4198\n",
      "Epoch 2/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 158ms/step - accuracy: 0.9333 - loss: 0.1793\n",
      "Epoch 3/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 160ms/step - accuracy: 0.9574 - loss: 0.1195\n",
      "Epoch 4/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 162ms/step - accuracy: 0.9728 - loss: 0.0867\n",
      "Epoch 5/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 167ms/step - accuracy: 0.9801 - loss: 0.0583\n",
      "GRU Performance\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.8488 - loss: 0.4222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4430953562259674, 0.8402500152587891]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300, input_shape = (X.shape[1],)))\n",
    "# takes in input dim = max uniqe values, it can be no longer than the highest tokenized index or max unique values. 2d tensor (batch, input_dim)\n",
    "# outputs output of length 120, 3D tensor (batch, input_shape or max_text_length, output_dim). Batch = None\n",
    "\n",
    "# It will output texts with max_text_length size each embedded to 120 dimensions.\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "# inputs 3D tensor, drops 40% input units.\n",
    "\n",
    "model.add(GRU(305, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# predict if 1 or 0. Binary \n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "# compile the model \n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Fitting on train\n",
    "batch_size=32\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 'auto')\n",
    "\n",
    "print('GRU Performance')\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad2ea5a",
   "metadata": {},
   "source": [
    "### BRNN - NO WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "4b7724ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p192\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_27\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_27\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">610</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,478,640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,222</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_27 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │       \u001b[38;5;34m5,050,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m610\u001b[0m)                 │       \u001b[38;5;34m1,478,640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │           \u001b[38;5;34m1,222\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,530,062</span> (24.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,530,062\u001b[0m (24.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,530,062</span> (24.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,530,062\u001b[0m (24.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 266ms/step - accuracy: 0.8344 - loss: 0.3849\n",
      "Epoch 2/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 322ms/step - accuracy: 0.9391 - loss: 0.1629\n",
      "Epoch 3/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 310ms/step - accuracy: 0.9668 - loss: 0.1008\n",
      "Epoch 4/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 323ms/step - accuracy: 0.9764 - loss: 0.0685\n",
      "Epoch 5/5\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 323ms/step - accuracy: 0.9817 - loss: 0.0540\n",
      "BRNN LSTM Performance\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 147ms/step - accuracy: 0.8785 - loss: 0.4289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4484785199165344, 0.8759999871253967]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300, input_shape = (X.shape[1],)))\n",
    "\n",
    "model.add(Bidirectional(LSTM(305, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# predict if 1 or 0. Binary \n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "# compile the model \n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Fitting on train\n",
    "batch_size=32\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 'auto')\n",
    "\n",
    "print('BRNN LSTM Performance')\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63927bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458e3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57354ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6ec96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e261e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62b98e1e",
   "metadata": {},
   "source": [
    "### Using Glove pretrained embedddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "447a7f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 14654 words (2179 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(word_index) + 2\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "89c67a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_22\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_22\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout1d_5                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">721,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">602</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_22 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │       \u001b[38;5;34m5,050,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout1d_5                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)                 │         \u001b[38;5;34m721,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m602\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,772,302</span> (22.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,772,302\u001b[0m (22.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,772,302</span> (22.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,772,302\u001b[0m (22.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 574ms/step - accuracy: 0.8254 - loss: 0.3893\n",
      "Epoch 2/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 561ms/step - accuracy: 0.8998 - loss: 0.2339\n",
      "Epoch 3/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 557ms/step - accuracy: 0.9326 - loss: 0.1661\n",
      "Epoch 4/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 555ms/step - accuracy: 0.9563 - loss: 0.1168\n",
      "Epoch 5/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 547ms/step - accuracy: 0.9667 - loss: 0.0915\n",
      "LSTM Performance\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 218ms/step - accuracy: 0.8996 - loss: 0.3124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31509363651275635, 0.8958333134651184]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index)+2, 300, input_shape = (X.shape[1],))\n",
    "embedding_layer.build((1,))\n",
    "embedding_layer.set_weights([embedding_matrix])\n",
    "\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "# inputs 3D tensor, drops 40% input units.\n",
    "\n",
    "model.add(LSTM(300, dropout=0.2, recurrent_dropout=0.2))\n",
    "# 3D tensor now fed to LSTM with 176 units. Returns (Batch, 176 outputs)\n",
    "# 20% Fraction of the units to drop for the linear transformation of the inputs.\n",
    "# 20% Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# predict if 1 or 0. Binary \n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "# compile the model \n",
    "\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#Splitting the data into training and testing\n",
    "y=pd.get_dummies(data_v1['sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Fitting on train\n",
    "batch_size=32\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 'auto')\n",
    "\n",
    "print('LSTM Performance')\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7d28d6",
   "metadata": {},
   "source": [
    "### BRNN - GOLVE WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "a693ab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\p192\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\core\\embedding.py:81: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_28\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_28\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout1d_8                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">610</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,478,640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,222</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_28 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │       \u001b[38;5;34m5,050,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ spatial_dropout1d_8                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m42\u001b[0m, \u001b[38;5;34m300\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m610\u001b[0m)                 │       \u001b[38;5;34m1,478,640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │           \u001b[38;5;34m1,222\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,530,362</span> (24.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,530,362\u001b[0m (24.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,530,362</span> (24.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,530,362\u001b[0m (24.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 274ms/step - accuracy: 0.8260 - loss: 0.3896\n",
      "Epoch 2/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 279ms/step - accuracy: 0.9039 - loss: 0.2337\n",
      "Epoch 3/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 281ms/step - accuracy: 0.9295 - loss: 0.1744\n",
      "Epoch 4/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 302ms/step - accuracy: 0.9521 - loss: 0.1201\n",
      "Epoch 5/5\n",
      "\u001b[1m438/438\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 518ms/step - accuracy: 0.9680 - loss: 0.0864\n",
      "BRNN GLOVE Performance\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 264ms/step - accuracy: 0.9009 - loss: 0.3019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.31185993552207947, 0.8970000147819519]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index)+2, 300, input_shape = (X.shape[1],))\n",
    "embedding_layer.build((1,))\n",
    "embedding_layer.set_weights([embedding_matrix])\n",
    "\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "# inputs 3D tensor, drops 40% input units.\n",
    "\n",
    "model.add(Bidirectional(LSTM(305, dropout=0.2, recurrent_dropout=0.2)))\n",
    "# 3D tensor now fed to LSTM with 176 units. Returns (Batch, 176 outputs)\n",
    "# 20% Fraction of the units to drop for the linear transformation of the inputs.\n",
    "# 20% Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# predict if 1 or 0. Binary \n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "# compile the model \n",
    "\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#Splitting the data into training and testing\n",
    "y=pd.get_dummies(data_v1['sentiment'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "# Fitting on train\n",
    "batch_size=32\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 'auto')\n",
    "\n",
    "print('BRNN GLOVE Performance')\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e3a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fe99a706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16834"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "baa6fa21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 42)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dfa609",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90030c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3e28d1d",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09ef69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6d9a0c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")     \n",
    "        \n",
    "        #will define weights and biases of size(out_dimensions from lstm, 1)\n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(np.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "        # here we do inner dot product of input with weights and biases and pass over tan and softmax which give alignment scores\n",
    "        #whose dimensions will be max_len = input_shape, 42 which will be dot product with input to get context vector.\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300, input_shape = (X.shape[1],)))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "# inputs 3D tensor, drops 40% input units.\n",
    "\n",
    "model.add(LSTM(305, dropout=0.2, recurrent_dropout=0.2, return_sequences = True))\n",
    "model.add(attention())\n",
    "# 3D tensor now fed to LSTM with 176 units. Returns (Batch, 176 outputs)\n",
    "# 20% Fraction of the units to drop for the linear transformation of the inputs.\n",
    "# 20% Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "# predict if 1 or 0. Binary \n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "# compile the model \n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Fitting on train\n",
    "batch_size=32\n",
    "model.fit(X_train, y_train, epochs = 5, batch_size=batch_size, verbose = 'auto')\n",
    "\n",
    "print('LSTM Performance')\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8db80d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7786e80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42,)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.shape[1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326f166",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7823f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe3f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9d892",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebafb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e3639",
   "metadata": {},
   "source": [
    "Step 4: Preprocessing cleaning\n",
    "\n",
    "Step 4: Tokenization & Encoding\n",
    "BERT tokenization is used to convert the raw text into numerical inputs that can be fed into the BERT model. It tokenized the text and performs some preprocessing to prepare the text for the model’s input format. Let’s understand some of the key features of the BERT tokenization model.\n",
    "\n",
    "BERT tokenizer splits the words into subwords or workpieces. For example, the word “geeksforgeeks” can be split into “geeks” “##for”, and”##geeks”. The “##” prefix indicates that the subword is a continuation of the previous one. It reduces the vocabulary size and helps the model to deal with rare or unknown words.\n",
    "\n",
    "BERT tokenizer adds special tokens like [CLS], [SEP], and [MASK] to the sequence. These tokens have special meanings like :\n",
    "\n",
    "[CLS] is used for classifications and to represent the entire input in the case of sentiment analysis,\n",
    "[SEP] is used as a separator i.e. to mark the boundaries between different sentences or segments,\n",
    "[MASK] is used for masking i.e. to hide some tokens from the model during pre-training.\n",
    "\n",
    "BERT tokenizer gives their components as outputs:\n",
    "\n",
    "input_ids: numerical tags to vocabulary tokens\n",
    "token_type_ids: It identifies which segment or sentence each token belongs to.\n",
    "attention_mask: It flags that inform the model which tokens to pay attention to and which to disregard.\n",
    "\n",
    "When training language models, defining a prediction goal is a challenge. Many models predict the next word in a sequence, which is a directional approach and may limit context learning. BERT addresses this challenge with two innovative training strategies:\n",
    "\n",
    "Masked Language Model (MLM)\n",
    "Next Sentence Prediction (NSP)\n",
    "\n",
    "\n",
    "1) MLM\n",
    "\n",
    "the real challenge is figuring out the missing parts, and this strategy helps BERT become really good at understanding the meaning and context of words.\n",
    "\n",
    "It masks around 15% tokens and model aims at guessing this word. \n",
    "\n",
    "The output vectors from the classification layer are multiplied by the embedding matrix, transforming them into the vocabulary dimension. This step helps align the predicted representations with the vocabulary space.\n",
    "\n",
    "encoder output is passed over softmax to make probabilities over entire vocabulary for masked position.\n",
    "\n",
    "The model converges slower than directional models. This is because, during training, BERT is only concerned with predicting the masked values, ignoring the prediction of the non-masked words. The increased context awareness achieved through this strategy compensates for the slower convergence.\n",
    "\n",
    "2) NSP\n",
    "model predicts if second sentence is follweed by first.\n",
    "\n",
    "<cls> sent1 <sep> sent2\n",
    "model's output predicts cls probabilty in 2*1 shaped vector. \n",
    "    \n",
    "During training, 50% correct pairs and 50% random pairs are fed and trained.\n",
    "    \n",
    "A sentence embedding indicating Sentence A or Sentence B is added to each token. A positional encoding is also added.\n",
    "    \n",
    "    \n",
    "NLM and NSP are trained simultaneously.\n",
    "    \n",
    "NER - each token gets convered to vectors and tags are asscoiated.\n",
    "    \n",
    "question answering - BERT is trained for question answering by learning two additional vectors that mark the beginning and end of the answer. During training, the model is provided with questions and corresponding passages, and it learns to predict the start and end positions of the answer within the passage.\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6570b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports libs\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd841239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and encode the data using the BERT tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) #uncased version lowers words before feeding to wordtokenizer\n",
    "\n",
    "\n",
    "max_len= 128\n",
    "# Tokenize and encode the sentences\n",
    "X_train_encoded = tokenizer.batch_encode_plus(Reviews.tolist(),\n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')\n",
    " \n",
    "X_val_encoded = tokenizer.batch_encode_plus(x_val.tolist(), \n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')\n",
    " \n",
    "X_test_encoded = tokenizer.batch_encode_plus(x_test.tolist(), \n",
    "                                              padding=True, \n",
    "                                              truncation=True,\n",
    "                                              max_length = max_len,\n",
    "                                              return_tensors='tf')\n",
    "\n",
    "\n",
    "k = 0\n",
    "print('Training Comments -->>',Reviews[k])\n",
    "\n",
    "'''Valentine is a horrible movie This is what I thought of itActing Very bad Katherine Heigl can not act \n",
    "The other's weren't much betterStory The story was okay, but it could have been more developed This movie had the potential to be a great movie, \n",
    "but it failedMusic Yes, some of the music was pretty coolOriginality Not very original The name Paige Prescott' Recognize PrescottBottom Line \n",
    "Don't see Valentine It's a really stupid movie110'''\n",
    "\n",
    "print('\\nInput Ids -->>\\n',X_train_encoded['input_ids'][k])\n",
    "'''\n",
    " tf.Tensor(\n",
    "[  101 10113  2003  1037  9202  3185  2023  2003  2054  1045  2245  1997\n",
    "  2009 18908  2075  2200  2919  9477  2002  8004  2140  2064  2025  2552\n",
    "  1996  2060  1005  1055  4694  1005  1056  2172  2488 23809  2100  1996\n",
    "  2466  2001  3100  1010  2021  2009  2071  2031  2042  2062  2764  2023\n",
    "  3185  2018  1996  4022  2000  2022  1037  2307  3185  1010  2021  2009\n",
    "  3478 27275  2748  1010  2070  1997  1996  2189  2001  3492  4658 10050\n",
    " 24965  3012  2025  2200  2434  1996  2171 17031 20719  1005  6807 20719\n",
    " 18384 20389  2240  2123  1005  1056  2156 10113  2009  1005  1055  1037\n",
    "  2428  5236  3185 14526  2692   102     0     0     0     0     0     0\n",
    "     0     0     0     0     0     0     0     0     0     0     0     0\n",
    "     0     0     0     0     0     0     0     0], shape=(128,), dtype=int32)\n",
    "'''\n",
    "\n",
    "print('\\nDecoded Ids -->>\\n',tokenizer.decode(X_train_encoded['input_ids'][k]))\n",
    "'''\n",
    "[CLS] valentine is a horrible movie this is what i thought of itacting very bad katherine heigl can not act \n",
    "the other's weren't much betterstory the story was okay, but it could have been more developed this movie had the potential to be a great movie,\n",
    " but it failedmusic yes, some of the music was pretty cooloriginality not very original the name paige prescott'recognize prescottbottom line \n",
    "don't see valentine it's a really stupid movie110 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
    "[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
    "'''\n",
    "\n",
    "print('\\nAttention Mask -->>\\n',X_train_encoded['attention_mask'][k])\n",
    "'''\n",
    " tf.Tensor(\n",
    "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
    " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
    " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
    " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(128,), dtype=int32)\n",
    "'''\n",
    "\n",
    "print('\\nLabels -->>',Target[k])\n",
    "'''\n",
    "Labels -->> 0\n",
    "'''\n",
    "\n",
    "# Intialize the model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compile the model with an appropriate optimizer, loss function, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "\n",
    "# Step 5: Train the model\n",
    "history = model.fit(\n",
    "    [X_train_encoded['input_ids'], X_train_encoded['token_type_ids'], X_train_encoded['attention_mask']],\n",
    "    Target,\n",
    "    validation_data=(\n",
    "      [X_val_encoded['input_ids'], X_val_encoded['token_type_ids'], X_val_encoded['attention_mask']],y_val),\n",
    "    batch_size=32,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "\n",
    "Epoch 1/3\n",
    "782/782 [==============================] - 513s 587ms/step - loss: 0.3445 - accuracy: 0.8446 - val_loss: 0.2710 - val_accuracy: 0.8880\n",
    "Epoch 2/3\n",
    "782/782 [==============================] - 432s 552ms/step - loss: 0.2062 - accuracy: 0.9186 - val_loss: 0.2686 - val_accuracy: 0.8886\n",
    "Epoch 3/3\n",
    "782/782 [==============================] - 431s 551ms/step - loss: 0.1105 - accuracy: 0.9615 - val_loss: 0.3235 - val_accuracy: 0.8908\n",
    "\n",
    "\n",
    "#Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    [X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']],\n",
    "    y_test\n",
    ")\n",
    "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')\n",
    "\n",
    "391/391 [==============================] - 67s 171ms/step - loss: 0.3417 - accuracy: 0.8873\n",
    "Test loss: 0.3417432904243469, Test accuracy: 0.8872799873352051\n",
    "        \n",
    "        \n",
    "# Save the mdel and tokenizer to local\n",
    "path = 'path-to-save'\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(path +'/Tokenizer')\n",
    "# Save model\n",
    "model.save_pretrained(path +'/Model')\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(path +'/Tokenizer')\n",
    "# Load model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(path +'/Model')\n",
    "\n",
    "# Predict the sentiment of test dataset\n",
    "\n",
    "pred = bert_model.predict(\n",
    "    [X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']])\n",
    " \n",
    "# pred is of type TFSequenceClassifierOutput\n",
    "logits = pred.logits\n",
    " \n",
    "# Use argmax along the appropriate axis to get the predicted labels\n",
    "pred_labels = tf.argmax(logits, axis=1)\n",
    " \n",
    "# Convert the predicted labels to a NumPy array\n",
    "pred_labels = pred_labels.numpy()\n",
    " \n",
    "label = {\n",
    "    1: 'positive',\n",
    "    0: 'Negative'\n",
    "}\n",
    " \n",
    "# Map the predicted labels to their corresponding strings using the label dictionary\n",
    "pred_labels = [label[i] for i in pred_labels]\n",
    "Actual = [label[i] for i in y_test]\n",
    " \n",
    "print('Predicted Label :', pred_labels[:10])\n",
    "print('Actual Label    :', Actual[:10])\n",
    "\n",
    "391/391 [==============================] - 68s 167ms/step\n",
    "Predicted Label : ['positive', 'positive', 'positive', 'positive', 'positive', 'Negative', 'Negative', 'Negative', 'positive', 'positive']\n",
    "Actual Label    : ['positive', 'positive', 'positive', 'Negative', 'positive', 'Negative', 'Negative', 'Negative', 'positive', 'positive']\n",
    "    \n",
    "print(\"Classification Report: \\n\", classification_report(Actual, pred_labels))\n",
    "\n",
    "Classification Report: \n",
    "               precision    recall  f1-score   support\n",
    "    Negative       0.91      0.86      0.88      6250\n",
    "    positive       0.87      0.91      0.89      6250\n",
    "    accuracy                           0.89     12500\n",
    "    macro avg      0.89      0.89      0.89     12500\n",
    "weighted avg       0.89      0.89      0.89     12500\n",
    "\n",
    "\n",
    "\n",
    "Step 7: Prediction with user inputs\n",
    "\n",
    "\n",
    "\n",
    "def Get_sentiment(Review, Tokenizer=bert_tokenizer, Model=bert_model):\n",
    "    # Convert Review to a list if it's not already a list\n",
    "    if not isinstance(Review, list):\n",
    "        Review = [Review]\n",
    " \n",
    "    Input_ids, Token_type_ids, Attention_mask = Tokenizer.batch_encode_plus(Review,\n",
    "                                                                             padding=True,\n",
    "                                                                             truncation=True,\n",
    "                                                                             max_length=128,\n",
    "                                                                             return_tensors='tf').values()\n",
    "    prediction = Model.predict([Input_ids, Token_type_ids, Attention_mask])\n",
    " \n",
    "    # Use argmax along the appropriate axis to get the predicted labels\n",
    "    pred_labels = tf.argmax(prediction.logits, axis=1)\n",
    " \n",
    "    # Convert the TensorFlow tensor to a NumPy array and then to a list to get the predicted sentiment labels\n",
    "    pred_labels = [label[i] for i in pred_labels.numpy().tolist()]\n",
    "    return pred_labels\n",
    "\n",
    "\n",
    "\n",
    "Review ='''Bahubali is a blockbuster Indian movie that was released in 2015. \n",
    "It is the first part of a two-part epic saga that tells the story of a legendary hero who fights for his kingdom and his love. \n",
    "The movie has received rave reviews from critics and audiences alike for its stunning visuals, \n",
    "spectacular action scenes, and captivating storyline., 'negative'''\n",
    "\n",
    "\n",
    "Get_sentiment(Review)\n",
    "\n",
    "1/1 [==============================] - 3s 3s/step\n",
    "['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224321a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d66c9d0d",
   "metadata": {},
   "source": [
    "Accuracy of pre-trained bert-base-uncased Model on raw text data: 92.00%\n",
    "Accuracy of pre-trained bert-base-uncased Model on pre-processed data: 89.00%\n",
    "\n",
    "When I trained the above model on pre-processed data, I was getting an accuracy of 89%, but on the raw text data and with early stopping, I got an accuracy of 92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba336d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
